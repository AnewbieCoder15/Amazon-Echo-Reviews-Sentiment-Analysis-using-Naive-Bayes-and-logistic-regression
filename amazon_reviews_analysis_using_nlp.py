# -*- coding: utf-8 -*-
"""Amazon Reviews Analysis Using NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RuzAcdHaA5Puqrm7M7npC6oAedEthGQA

# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE

![image.png](attachment:image.png)

![image.png](attachment:image.png)

data source: https://www.kaggle.com/sid321axn/amazon-alexa-reviews/kernels

# TASK #2: IMPORT LIBRARIES AND DATASETS
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Load the data
reviews_df =  pd.read_csv('/content/amazon_reviews.csv')
reviews_df.sample(10)

# View the DataFrame Information
reviews_df.info()

# View DataFrame Statistical Summary
reviews_df.describe()

"""**MINI CHALLENGE #1:** 
- **Drop the 'date' column from the DataFrame** 
- **Ensure that the column has been succesfully dropped** 
"""

reviews_df.drop('date',axis=1,inplace=True)

reviews_df

"""# TASK #3: PERFORM DATA VISUALIZATION"""

sns.heatmap(reviews_df.isnull(),yticklabels=False,cmap='Blues')

# Plot the count plot for the ratings
sns.countplot(x='rating',data=reviews_df)

"""**MINI CHALLENGE #2:** 
- **Plot the countplot for the feedback column**
- **Roughly how many positive and negative feedback are present in the dataset?**
"""

sns.countplot(x='feedback',data=reviews_df)

reviews_df['feedback'].value_counts()

"""# TASK #4: PERFORM DATA EXPLORATION"""

# Let's get the length of the verified_reviews column
reviews_df['length']= reviews_df['verified_reviews'].apply(len)

reviews_df

# Plot the histogram for the length
reviews_df['length'].hist(bins=100)

# Apply the describe method to get statistical summary
reviews_df.describe()

# Let's see the longest message 
reviews_df[reviews_df['length']==reviews_df['length'].max()]['verified_reviews'].iloc[0]

"""**MINI CHALLENGE #3:**
- **View the message with the average length**
"""

reviews_df[reviews_df['length']==round(reviews_df['length'].mean())]['verified_reviews'].iloc[0]

"""# TASK #5: PLOT THE WORDCLOUD"""

# Obtain only the positive reviews
positive = reviews_df[reviews_df['feedback']==1]

positive

# Obtain the negative reviews only
negetive = reviews_df[reviews_df['feedback']==0]

negetive

# Convert to list format
len(list(positive['verified_reviews']))

# Join all reviews into one large string
positive_sentences = ' '.join(list(positive['verified_reviews']))

positive_sentences

from wordcloud import WordCloud
plt.figure(figsize=(20,20))
plt.imshow(WordCloud(max_words=2000).generate(positive_sentences))

"""**MINI CHALLENGE #4:** 
- **Plot the wordcloud of the "negative" dataframe** 
- **What do you notice? Does the data make sense?**
"""

plt.figure(figsize=(20,20))
plt.imshow(WordCloud(max_words=2000).generate(' '.join(list(negetive['verified_reviews']))))

"""# TASK #6: TEXT DATA CLEANING 101"""

import string
string.punctuation

Test = '$Hey, I am Suman Das...!!!!'

[ char for char in Test if char not in string.punctuation ]

# Join the characters again to form the string.
Test_punc_removed_join = ''.join([ char for char in Test if char not in string.punctuation ])
Test_punc_removed_join

import nltk # Natural Language tool kit

# You have to download stopwords Package to execute this command
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords.words('english')

Test_punc_removed_join = 'I have been enjoying these coding, programming and AI Projects on Rhyme and Coursera'

Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]

Test_punc_removed_join_clean

"""**MINI CHALLENGE #5:** 
- **For the following text, create a pipeline to remove punctuations followed by removing stopwords and test the pipeline**
- **mini_challenge = 'Here is a mini challenge, that will teach you how to remove stopwords and punctuations from text..!!'**
"""

mini_challenge = 'Here is a mini challenge, that will teach you how to remove stopwords and punctuations from text..!!'

def clean_text(text):
  punctuation_free_text = ''.join([ char for char in text if char not in string.punctuation ])
  stopwords_free_text = ' '.join([ word for word in punctuation_free_text.split() if word.lower() not in stopwords.words('english') ])
  return stopwords_free_text
print(clean_text(mini_challenge))

"""# TASK #7: PERFORM COUNT VECTORIZATION (TOKENIZATION)

![image.png](attachment:image.png)
"""

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first paper.','This document is the second paper.','And this is the third one.','Is this the first paper?']
vectorizer = CountVectorizer()
countvectorizer=vectorizer.fit_transform(sample_data)

print(vectorizer.get_feature_names_out())

print(countvectorizer.toarray())

"""**MINI CHALLENGE #6:**
- **Without doing any code, perform count vectorization for the following list:**
    -  mini_challenge = ['Hello World','Hello Hello World','Hello World world world']
- **Confirm your answer with code**
"""

vectorizer = CountVectorizer()
countvectorizer=vectorizer.fit_transform(['Hello World','Hello Hello World','Hello World world world'])
print(vectorizer.get_feature_names_out())
print(countvectorizer.toarray())

"""# TASK #8: CREATE A PIPELINE TO REMOVE PUNCTUATIONS, STOPWORDS AND PERFORM COUNT VECTORIZATION"""

# Let's define a pipeline to clean up all the messages 
# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords
def message_cleaning(text):
  punctuation_free_text = ''.join([ char for char in text if char not in string.punctuation ])
  ans = [ word for word in punctuation_free_text.split() if word.lower() not in stopwords.words('english')]
  return ans

# Let's test the newly added function
reviews_df_clean = reviews_df['verified_reviews'].apply(message_cleaning)

# show the original review
print(reviews_df['verified_reviews'][5])

# show the cleaned up version
print(reviews_df_clean[5])

from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning)
reviews_countvectorizer = vectorizer.fit_transform(reviews_df['verified_reviews'])

print(vectorizer.get_feature_names_out())

print(reviews_countvectorizer.toarray())

reviews_countvectorizer.shape

reviews = pd.DataFrame(reviews_countvectorizer.toarray())

X = reviews

X

y = reviews_df['feedback']
y

"""**MINI CHALLENGE #7:**
- **What is the shape of X and Y**
"""

print('Shape of input features : ',X.shape)
print('Shape of output label : ',y.shape)

"""# TASK #9: TRAIN AND TEST NAIVE BAYES CLASSIFIER MODEL"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB
NB_classifier = MultinomialNB()
NB_classifier.fit(X_train,y_train)

"""![image.png](attachment:image.png)"""

from sklearn.metrics import classification_report, confusion_matrix

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
sns.heatmap(confusion_matrix(y_test,y_predict_test),annot=True,cmap='Blues',cbar=True)

print(classification_report(y_test,y_predict_test))

"""**MINI CHALLENGE #8:**
- **Train a logistic Regression classifier and assess its performance**
"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

cm = confusion_matrix(y_pred, y_test)
sns.heatmap(cm, annot = True,cmap='Blues')

print(classification_report(y_test, y_pred))

